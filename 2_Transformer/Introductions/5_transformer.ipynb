{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdde29f8",
   "metadata": {},
   "source": [
    "Detailed structure can be found online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb85a9a",
   "metadata": {},
   "source": [
    "### Key Padding Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c001c",
   "metadata": {},
   "source": [
    "### Attention Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dded822",
   "metadata": {},
   "source": [
    "### KV Caching\n",
    "\n",
    "First, recall that the attention mechanism produces one value for each query, no matter how many key-value pairs exist.\n",
    "        \n",
    "During our decoder's self-attention, the input sequence simultaneously act as queries, keys, and values.\n",
    "So self-attention will always produce output that has the same length with the input sequence.\n",
    "Also during decoder's cross-attention, the input sequence are queries, and the encoder's output are keys and values,\n",
    "so it also retains input sequence length.\n",
    "\n",
    "Normally to build up output sequence, we feed current sequence into decoder every time, producing a new sequence with same legnth.\n",
    "For example, to produce \"it is a nice day\", we might feed \"it is a nice\" into decoder, and get \"is a nice day\" as output.\n",
    "One obvious drawback of this approach is that we are recomputing a lot of tokens, such as \"is\" \"a\" and \"nice\",\n",
    "when we really just want to compute the new token \"day\".\n",
    "\n",
    "The solution is to have a KV Cache storing current predicted sequence, such as \"it is a\".\n",
    "After the model produced \"nice\", we add it to our cache, and use cache \"it is a nice\" as key and value during self-attention.\n",
    "We still use encoder's output as cross-attention's key and value.\n",
    "Then, we only input \"nice\" as query to both attentions, producing a single-token output, which we expect to be \"day\"."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
