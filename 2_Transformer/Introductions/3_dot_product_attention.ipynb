{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047255f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd716b",
   "metadata": {},
   "source": [
    "### Dot Product Attention\n",
    "\n",
    "The setting in NW Regression has queries, keys and values of dimension one.\n",
    "\n",
    "More generally, we have multiple dimensional data, represented by row vectors (assume $\\mathbf{q}$, $\\mathbf{k}_i$ are of same dimension):\n",
    "\n",
    "$$\\mathbf{q}, \\mathbf{k}_i \\in \\mathbf{R}^{1\\times d}, \\mathbf{v}_j \\in \\mathbf{R}^{1\\times v}$$\n",
    "\n",
    "When scaled dot product is used as attention algorithm, we have scalar attention weight (before normalization):\n",
    "\n",
    "$$a(\\mathbf{q}, \\mathbf{k}_i) = \\frac{\\mathbf{q} \\cdot \\mathbf{k}_i}{\\sqrt{d}} = \\frac{\\mathbf{q} \\mathbf{k}^T_i}{\\sqrt{d}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e78309",
   "metadata": {},
   "source": [
    "\n",
    "For a database with $m$ keys (and values), we can place each key (and value) in a row vector, stacking up into a matrix:\n",
    "\n",
    "$$K = \\begin{bmatrix} \\mathbf{k}_1 \\\\ \\mathbf{k}_2 \\\\ \\dots \\\\ \\mathbf{k}_m \\end{bmatrix} \\in \\mathbf{R}^{m \\times d}, V = \\begin{bmatrix} \\mathbf{v}_1 \\\\ \\mathbf{v}_2 \\\\ \\dots \\\\ \\mathbf{v}_m \\end{bmatrix} \\in \\mathbf{R}^{m \\times v}$$\n",
    "\n",
    "Now, to perform a single query (represented by row vector $\\mathbf{q}$), we first calculate the attention scores on all keys:\n",
    "\n",
    "$$\\mathbf{a} = \\begin{bmatrix}\n",
    "a_1 & a_2 & \\dots & a_m\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\mathbf{q} \\mathbf{k}_1^T}{\\sqrt{d}} & \\frac{\\mathbf{q} \\mathbf{k}_2^T}{\\sqrt{d}} & \\dots & \\frac{\\mathbf{q} \\mathbf{k}_m^T}{\\sqrt{d}}\n",
    "\\end{bmatrix} = \\frac{\\mathbf{q} K^T}{\\sqrt{d}}$$\n",
    "\n",
    "Then we apply softmax to get the attention weights:\n",
    "\n",
    "$$\\mathbf{w} = \\begin{bmatrix}\n",
    "w_1 & w_2 & \\dots & w_m\n",
    "\\end{bmatrix} = softmax( \\begin{bmatrix}\n",
    "\\frac{\\mathbf{q} \\mathbf{k}_1^T}{\\sqrt{d}} & \\frac{\\mathbf{q} \\mathbf{k}_2^T}{\\sqrt{d}} & \\dots & \\frac{\\mathbf{q} \\mathbf{k}_m^T}{\\sqrt{d}}\n",
    "\\end{bmatrix}) = softmax(\\frac{\\mathbf{q} K^T}{\\sqrt{d}})$$\n",
    "\n",
    "To get the output, we take the weighted sum of all values $\\mathbf{v}_j$, which is just:\n",
    "\n",
    "$$\\hat{\\mathbf{v}} = w_1 \\mathbf{v}_1 + w_2 \\mathbf{v}_2 + \\dots + w_m \\mathbf{v}_m = \\mathbf{w} V = softmax(\\frac{\\mathbf{q} K^T}{\\sqrt{d}}) V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da8512",
   "metadata": {},
   "source": [
    "When trying to perform $n$ queries at the same time, we have a matrix of queries:\n",
    "\n",
    "$$Q = \\begin{bmatrix} \\mathbf{q}_1 \\\\ \\mathbf{q}_2 \\\\ \\dots \\\\ \\mathbf{q}_n \\end{bmatrix} \\in \\mathbf{R}^{n \\times d}$$\n",
    "\n",
    "For each $\\mathbf{q}_i$, attention weights are the same as as above:\n",
    "\n",
    "$$\\mathbf{a}_i = \\begin{bmatrix}\n",
    "a_{i1} & a_{i2} & \\dots & a_{im}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\mathbf{q}_i \\mathbf{k}_1^T}{\\sqrt{d}} & \\frac{\\mathbf{q}_i \\mathbf{k}_2^T}{\\sqrt{d}} & \\dots & \\frac{\\mathbf{q}_i \\mathbf{k}_m^T}{\\sqrt{d}}\n",
    "\\end{bmatrix} = \\frac{\\mathbf{q}_i K^T}{\\sqrt{d}}$$\n",
    "\n",
    "$$\\mathbf{w}_i = \\begin{bmatrix}\n",
    "w_{i1} & w_{i2} & \\dots & w_{im}\n",
    "\\end{bmatrix} = softmax( \\begin{bmatrix}\n",
    "\\frac{\\mathbf{q}_i \\mathbf{k}_1^T}{\\sqrt{d}} & \\frac{\\mathbf{q}_i \\mathbf{k}_2^T}{\\sqrt{d}} & \\dots & \\frac{\\mathbf{q}_i \\mathbf{k}_m^T}{\\sqrt{d}}\n",
    "\\end{bmatrix}) = softmax(\\frac{\\mathbf{q}_i K^T}{\\sqrt{d}})$$\n",
    "\n",
    "\n",
    "Stacking them up, the weight matrix for these queries are actually calculated as:\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "\\mathbf{a}_1 \\\\\n",
    "\\mathbf{a}_2 \\\\\n",
    "\\dots \\\\\n",
    "\\mathbf{a}_n\n",
    "\\end{bmatrix} = \\frac{QK^T}{\\sqrt{d}} \\in \\mathbf{R}^{n \\times m}$$\n",
    "\n",
    "$$W = \\begin{bmatrix}\n",
    "\\mathbf{w}_1 \\\\\n",
    "\\mathbf{w}_2 \\\\\n",
    "\\dots \\\\\n",
    "\\mathbf{w}_n\n",
    "\\end{bmatrix} = Softmax(\\frac{QK^T}{\\sqrt{d}}) \\in \\mathbf{R}^{n \\times m}$$\n",
    "\n",
    "where `Softmax` applies softmax to each row in the matrix.\n",
    "\n",
    "Now we have the complete prediction for the queries:\n",
    "\n",
    "$$\\hat{V} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{\\mathbf{v}}_1 \\\\\n",
    "\\hat{\\mathbf{v}}_2 \\\\\n",
    "\\dots \\\\\n",
    "\\hat{\\mathbf{v}}_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{w}_1 V \\\\\n",
    "\\mathbf{w}_2 V \\\\\n",
    "\\dots \\\\\n",
    "\\mathbf{w}_n V\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w_{11} \\mathbf{v}_1 + w_{12} \\mathbf{v}_2 + \\dots + w_{1m} \\mathbf{v}_m \\\\\n",
    "w_{21} \\mathbf{v}_1 + w_{22} \\mathbf{v}_2 + \\dots + w_{2m} \\mathbf{v}_m \\\\\n",
    "\\dots \\\\\n",
    "w_{n1} \\mathbf{v}_1 + w_{n2} \\mathbf{v}_2 + \\dots + w_{nm} \\mathbf{v}_m \\\\\n",
    "\\end{bmatrix}\n",
    "= WV = Softmax(\\frac{QK^T}{\\sqrt{d}})V \\in \\mathbf{R}^{n \\times v}$$\n",
    "\n",
    "For convenience, we denote $f$ as the attention pooling operation from now on:\n",
    "\n",
    "$$f(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d}}) V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b10b40",
   "metadata": {},
   "source": [
    "### Batch Matrix Multiplication\n",
    "\n",
    "When our task is to perform a batch (size $b$) of attention pooling, each containing $n$ queries, we perform multiple matmuls at the same time:\n",
    "\n",
    "$$BMM(\\mathbf{Q}_{\\text{batch}} \\in \\mathbf{R}^{b \\times n \\times d}, \\mathbf{K}^T_{\\text{batch}} \\in \\mathbf{R}^{b \\times d \\times m}) \\in \\mathbf{R}^{b \\times n \\times m}$$\n",
    "\n",
    "GPUs come in handy as they can compute multiple matmuls at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511529fd",
   "metadata": {},
   "source": [
    "### Masked Softmax\n",
    "\n",
    "In many tasks, different queries might operate on different databases of different sizes, since sentences might be of different lengths.\n",
    "\n",
    "This means that for each query (total of n), database length $m$ is different, resulting in attention scores of different lengths, such as:\n",
    "\n",
    "$$A =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Even for the same database, you might want different queries to access different entries in it.\n",
    "\n",
    "Such as in Transformer's decoder, database is the target sentence, and query is the current token.\n",
    "\n",
    "Each token should only see the generated tokens so far, meaning each query should only access database up to it:\n",
    "\n",
    "$$A =\n",
    "\\begin{bmatrix}\n",
    "a_{11} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "In either case, place-holders with no actual meaning are padded to make calculation consistent:\n",
    "\n",
    "$$A =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "To ensure place-holders don't contribute to output or gradient, we need to have the padded weights output zero after taking softmax:\n",
    "\n",
    "$$w_{13}= \\alpha(\\mathbf{q}_1, \\mathbf{k}_3) = softmax(a(\\mathbf{q}_1, \\mathbf{k}_3)) = 0$$\n",
    "\n",
    "which can be done by having $a_{13}$ equal to `-inf`.\n",
    "\n",
    "In other words, we want to mask an attention score matrix A so that it looks like:\n",
    "\n",
    "$$A =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & -\\inf \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & -\\inf & -\\inf\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362f4b6",
   "metadata": {},
   "source": [
    "In practice, masked attention functions take in an argument `key_padding_mask` specifying the valid length $m$ for each query.\n",
    "\n",
    "To achieve the above masking, `key_padding_mask` would be $\\begin{bmatrix} 2 & 3 & 1 \\end{bmatrix}$.\n",
    "\n",
    "After original attention scores\n",
    "\n",
    "$$A =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "are calculated, we create a masking matrix according to `key_padding_mask`\n",
    "\n",
    "$$M =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & -\\inf \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & -\\inf & -\\inf\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "and add it to the original attention score matrix, resulting in the masked attention score matrix A\n",
    "\n",
    "$$A =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & -\\inf \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & -\\inf & -\\inf\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "After this, taking softmax will produce desired attention weights\n",
    "\n",
    "$$W =\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & 0 \\\\\n",
    "w_{21} & w_{22} & w_{23} \\\\\n",
    "w_{31} & 0 & 0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "For batches of queries, `key_padding_mask` would be in 2D form [[],[],...], each list for a batch of queries."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
