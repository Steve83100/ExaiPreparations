{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047255f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd716b",
   "metadata": {},
   "source": [
    "### Dot Product Attention\n",
    "\n",
    "The setting in NW Regression has queries, keys and values of dimension one. More generally (assume $\\mathbf{q}$, $\\mathbf{k}$ are of same dimension):\n",
    "\n",
    "$$\\mathbf{q}, \\mathbf{k} \\in \\mathbf{R}^d, \\mathbf{v} \\in \\mathbf{R}^v$$\n",
    "\n",
    "When scaled dot product is used as attention algorithm, we have scalar attention weight (before normalization):\n",
    "\n",
    "$$a(\\mathbf{q}, \\mathbf{k}) = \\frac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d}} = \\frac{\\mathbf{q} \\mathbf{k}^T}{\\sqrt{d}}$$\n",
    "\n",
    "To get the actual scalar attention weight, we have to normalize the above weights on all keys $\\mathbf{k}_i$:\n",
    "\n",
    "$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = softmax(a(\\mathbf{q}, \\mathbf{k}_i)) = softmax(\\frac{\\mathbf{q} \\mathbf{k}_i^T}{\\sqrt{d}}) = \\frac{\\frac{\\mathbf{q} \\mathbf{k}_i^T}{\\sqrt{d}}}{\\sum \\frac{\\mathbf{q} \\mathbf{k}_i^T}{\\sqrt{d}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e78309",
   "metadata": {},
   "source": [
    "\n",
    "For a database with $m$ keys (and values), we can place each key (and value) in a row vector, stacking up into a matrix:\n",
    "\n",
    "$$K = \\begin{bmatrix} \\mathbf{k}_1 \\\\ \\mathbf{k}_2 \\\\ \\dots \\\\ \\mathbf{k}_m \\end{bmatrix} \\in \\mathbf{R}^{m \\times d}, V = \\begin{bmatrix} \\mathbf{v}_1 \\\\ \\mathbf{v}_2 \\\\ \\dots \\\\ \\mathbf{v}_m \\end{bmatrix} \\in \\mathbf{R}^{m \\times v}$$\n",
    "\n",
    "Now, to perform a single query (represented by row vector $\\mathbf{q}$), we first calculate the attention weights on all keys (with normalization):\n",
    "\n",
    "$$\\mathbf{w} = \\begin{bmatrix}\n",
    "w_1 & w_2 & \\dots & w_m\n",
    "\\end{bmatrix} = softmax( \\begin{bmatrix}\n",
    "\\frac{\\mathbf{q} \\mathbf{k}_1^T}{\\sqrt{d}} & \\frac{\\mathbf{q} \\mathbf{k}_2^T}{\\sqrt{d}} & \\dots & \\frac{\\mathbf{q} \\mathbf{k}_m^T}{\\sqrt{d}}\n",
    "\\end{bmatrix}) = softmax(\\frac{\\mathbf{q} K^T}{\\sqrt{d}})$$\n",
    "\n",
    "To get the output, we take the weighted sum of all values $\\mathbf{v}_j$, which is just:\n",
    "\n",
    "$$\\hat{\\mathbf{v}} = w_1 \\mathbf{v}_1 + w_2 \\mathbf{v}_2 + \\dots + w_m \\mathbf{v}_m = \\mathbf{w} V = softmax(\\frac{\\mathbf{q} K^T}{\\sqrt{d}}) V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da8512",
   "metadata": {},
   "source": [
    "When trying to perform $n$ queries at the same time, we have a matrix of queries:\n",
    "\n",
    "$$Q = \\begin{bmatrix} \\mathbf{q}_1 \\\\ \\mathbf{q}_2 \\\\ \\dots \\\\ \\mathbf{q}_n \\end{bmatrix} \\in \\mathbf{R}^{n \\times d}$$\n",
    "\n",
    "For each $\\mathbf{q}_i$, we compute the same weight vector as above:\n",
    "\n",
    "$$\\mathbf{w}_i = \\begin{bmatrix}\n",
    "w_{i1} & w_{i2} & \\dots & w_{im}\n",
    "\\end{bmatrix} = softmax( \\begin{bmatrix}\n",
    "\\frac{\\mathbf{q}_i \\mathbf{k}_1^T}{\\sqrt{d}} & \\frac{\\mathbf{q}_i \\mathbf{k}_2^T}{\\sqrt{d}} & \\dots & \\frac{\\mathbf{q}_i \\mathbf{k}_m^T}{\\sqrt{d}}\n",
    "\\end{bmatrix}) = softmax(\\frac{\\mathbf{q}_i K^T}{\\sqrt{d}})$$\n",
    "\n",
    "\n",
    "Then the weight matrix for these queries is:\n",
    "\n",
    "$$W = \\begin{bmatrix}\n",
    "\\mathbf{w}_1 \\\\\n",
    "\\mathbf{w}_2 \\\\\n",
    "\\dots \\\\\n",
    "\\mathbf{w}_n\n",
    "\\end{bmatrix} = Softmax(\\frac{QK^T}{\\sqrt{d}}) \\in \\mathbf{R}^{n \\times m}$$\n",
    "\n",
    "where `Softmax` applies softmax to each row in the matrix.\n",
    "\n",
    "Now we have the complete prediction for the queries:\n",
    "\n",
    "$$\\hat{V} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{\\mathbf{v}}_1 \\\\\n",
    "\\hat{\\mathbf{v}}_2 \\\\\n",
    "\\dots \\\\\n",
    "\\hat{\\mathbf{v}}_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{w}_1 V \\\\\n",
    "\\mathbf{w}_2 V \\\\\n",
    "\\dots \\\\\n",
    "\\mathbf{w}_n V\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w_{11} \\mathbf{v}_1 + w_{12} \\mathbf{v}_2 + \\dots + w_{1m} \\mathbf{v}_m \\\\\n",
    "w_{21} \\mathbf{v}_1 + w_{22} \\mathbf{v}_2 + \\dots + w_{2m} \\mathbf{v}_m \\\\\n",
    "\\dots \\\\\n",
    "w_{n1} \\mathbf{v}_1 + w_{n2} \\mathbf{v}_2 + \\dots + w_{nm} \\mathbf{v}_m \\\\\n",
    "\\end{bmatrix}\n",
    "= WV = Softmax(\\frac{QK^T}{\\sqrt{d}})V \\in \\mathbf{R}^{n \\times v}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511529fd",
   "metadata": {},
   "source": [
    "### Masked Softmax\n",
    "\n",
    "In some tasks, different queries might operate on different databases of different sizes, having a different $m$ for each $n$, such as:\n",
    "\n",
    "$$\\hat{V} =\n",
    "\\begin{bmatrix}\n",
    "w_{n1} \\mathbf{v}_1 + w_{n2} \\mathbf{v}_2\\\\\n",
    "w_{21} \\mathbf{v}_1 + w_{22} \\mathbf{v}_2 + w_{23} \\mathbf{v}_3 + w_{24} \\mathbf{v}_4 \\\\\n",
    "w_{11} \\mathbf{v}_1 + w_{12} \\mathbf{v}_2 + w_{13} \\mathbf{v}_3\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "In this case, place-holders are padded to make calculation consistent:\n",
    "\n",
    "$$\\hat{V} =\n",
    "\\begin{bmatrix}\n",
    "w_{n1} \\mathbf{v}_1 + w_{n2} \\mathbf{v}_2 + \\tilde{w}_{23} \\tilde{\\mathbf{v}}_3 + \\tilde{w}_{24} \\tilde{\\mathbf{v}}_4 \\\\\n",
    "w_{21} \\mathbf{v}_1 + w_{22} \\mathbf{v}_2 + w_{23} \\mathbf{v}_3 + w_{24} \\mathbf{v}_4 \\\\\n",
    "w_{11} \\mathbf{v}_1 + w_{12} \\mathbf{v}_2 + w_{23} \\mathbf{v}_3 + \\tilde{w}_{24} \\tilde{\\mathbf{v}}_4\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "To ensure place-holders don't contribute to output or gradient, we need to have the padded weights output zero:\n",
    "\n",
    "$$w_{23}= \\alpha(\\mathbf{q}_2, \\mathbf{k}_3) = softmax(a(\\mathbf{q}_2, \\mathbf{k}_3)) = 0$$\n",
    "\n",
    "which can be done by having $a(\\mathbf{q}_2, \\mathbf{k}_3)$ equal to a very negative number, such as $10^{-6}$.\n",
    "\n",
    "`Masked_Softmax` takes in a second argument specifying the valid length $m$ for each query, and masks $10^{-6}$ before computing softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89ffa7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
    "    def _sequence_mask(X, valid_len, value=0):\n",
    "        maxlen = X.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                            device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
