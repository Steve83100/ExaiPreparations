{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45574320",
   "metadata": {},
   "source": [
    "## Representing Sequences\n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "Split the sequence into tokens. A token might contain a letter, a word, or many words.\n",
    "\n",
    "### Vocabulary\n",
    "\n",
    "Construct a vocabulary containing all unique tokens in the sequence, and represent each token with a number or a one-hot vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5befc20",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "Within a sequence, language models try to predict the next token from previously seen tokens.\n",
    "\n",
    "N-gram model assumes that only the previous $n-1$ tokens contribute to current token, following an nth-order markov property:\n",
    "\n",
    "$$P(x_t | x_{t-1}, \\dots, x_1) = P(x_t | x_{t-1}, \\dots, x_{t-n+1})$$\n",
    "\n",
    "To calculate the probability, there are different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df145b",
   "metadata": {},
   "source": [
    "### Word Frequency Approach\n",
    "\n",
    "The word frequency approach calculates probability simply by counting how many times certain word combinations appeared in the sequence:\n",
    "\n",
    "$$P(x_3 | x_2, x_1) = \\frac{n(x_3, x_2, x_1)}{n(x_2, x_1)}$$\n",
    "\n",
    "But this approach performs poorly if $n$ gets big, when novel word combinations appear more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc3a07b",
   "metadata": {},
   "source": [
    "### Neural Network Approach\n",
    "\n",
    "The neural network approach summarizes information from previous tokens into a hidden variable:\n",
    "\n",
    "$$h_{t-1} = f_{summary}(x_{t-1}, \\dots, x_{t-n+1})$$\n",
    "\n",
    "With this hidden variable added, possibility can be written as:\n",
    "\n",
    "$$P(x_t | x_{t-1}, \\dots, x_{t-n+1}) = P(x_t | h_{t-1})$$\n",
    "\n",
    "At each time step, the neural network does two calculations:\n",
    "\n",
    "1. Summarize a new hidden variable $h_t = f_{summary}(x_t, h_{t-1})$.\n",
    "\n",
    "2. Predict current token $P(x_t | h_{t-1}) = f_{predict}(h_t)$. Note that $h_t$ contains information from $x_t$ and $h_{t-1}$\n",
    "\n",
    "The above 2 functions are represented in the classic MLP structure (written in matrix form for batch calculation):\n",
    "\n",
    "$$H_t = \\phi(X_tW_{xh} + H_{t-1}W_{hh} + \\mathbf{b}_h)$$\n",
    "\n",
    "$$P_t = \\phi(H_tW_{hp} + \\mathbf{b}_p)$$\n",
    "\n",
    "Parameters $W, \\mathbf{b}$ are learned through training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55fba6",
   "metadata": {},
   "source": [
    "### Measuring Model Performance\n",
    "\n",
    "Perplexity is defined as:\n",
    "\n",
    "$$\\exp (-\\frac{1}{n} \\sum_{i=1}^n \\log P(x_t | x_{t-1}, \\dots, x_1))$$\n",
    "\n",
    "where $x_t$ is the actual label, and $P(x_t | x_{t-1}, \\dots, x_1)$ is the model's output of the possibility of $x_t$ being the current token.\n",
    "\n",
    "In best cases, the model predicts a possibility of 1 for all actual $x_t$, then perplexity equals 1.\n",
    "\n",
    "In worst cases, the model predicts a possibility of 0 for all actual $x_t$, then perplexity equals positive infinity."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
