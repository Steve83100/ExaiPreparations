{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45574320",
   "metadata": {},
   "source": [
    "## Representing Sequences\n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "Split the sequence into tokens. A token might contain a letter, a word, or many words.\n",
    "\n",
    "### Embedding\n",
    "\n",
    "Construct a vocabulary (size $v$) containing all unique tokens in the sequence, and represent each token with a number or a one-hot vector of size $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5befc20",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "Within a sequence, language models try to predict the next token from previously seen tokens.\n",
    "\n",
    "N-gram model assumes that only the previous $n-1$ tokens contribute to current token, following an nth-order markov property:\n",
    "\n",
    "$$P(x_t | x_{t-1}, \\dots, x_1) = P(x_t | x_{t-1}, \\dots, x_{t-n+1})$$\n",
    "\n",
    "To calculate the probability, there are different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df145b",
   "metadata": {},
   "source": [
    "### Word Frequency Approach\n",
    "\n",
    "The word frequency approach calculates probability simply by counting how many times certain word combinations appeared in the sequence:\n",
    "\n",
    "$$P(x_3 | x_2, x_1) = \\frac{n(x_3, x_2, x_1)}{n(x_2, x_1)}$$\n",
    "\n",
    "But this approach performs poorly if $n$ gets big, when novel word combinations appear more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc3a07b",
   "metadata": {},
   "source": [
    "### Neural Network Approach\n",
    "\n",
    "The neural network approach summarizes information from previous tokens into a hidden variable:\n",
    "\n",
    "$$h_{t-1} = f_{summary}(x_{t-1}, \\dots, x_{t-n+1})$$\n",
    "\n",
    "With this hidden variable added, possibility can be written as:\n",
    "\n",
    "$$P(x_t | x_{t-1}, \\dots, x_{t-n+1}) = P(x_t | h_{t-1})$$\n",
    "\n",
    "At each time step, the neural network does 2 calculations:\n",
    "\n",
    "1. Summarize a new hidden variable $h_t = f_{summary}(x_t, h_{t-1})$.\n",
    "\n",
    "2. Predict current token $o_t = P(x_t | h_{t-1}) = f_{predict}(h_t)$. Note that $h_t$ contains information from $x_t$ and $h_{t-1}$\n",
    "\n",
    "The above 2 functions are represented in the classic MLP structure (written in matrix form for batch calculation):\n",
    "\n",
    "$$H_t = \\phi(X_tW_{xh} + H_{t-1}W_{hh} + \\mathbf{b}_h)$$\n",
    "\n",
    "$$O_t = H_tW_{hp} + \\mathbf{b}_p$$\n",
    "\n",
    "Parameters $W, \\mathbf{b}$ are learned through training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55fba6",
   "metadata": {},
   "source": [
    "### Measuring Model Performance\n",
    "\n",
    "Perplexity is defined as:\n",
    "\n",
    "$$\\exp (-\\frac{1}{n} \\sum_{i=1}^n \\log P(x_t | x_{t-1}, \\dots, x_1))$$\n",
    "\n",
    "where $x_t$ is the actual label, and $P(x_t | x_{t-1}, \\dots, x_1)$ is the model's output of the possibility of $x_t$ being the current token.\n",
    "\n",
    "In best cases, the model predicts a possibility of 1 for all actual $x_t$, then perplexity equals 1.\n",
    "\n",
    "In worst cases, the model predicts a possibility of 0 for all actual $x_t$, then perplexity equals positive infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3d837",
   "metadata": {},
   "source": [
    "## Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de0b72b",
   "metadata": {},
   "source": [
    "A general structure for sequence-to-sequence (seq2seq) tasks.\n",
    "\n",
    "Encoder takes the input sequence of variable length $T$, computes a state $\\mathbf{c}$ of fixed length.\n",
    "\n",
    "Decoder takes the state from encoder and the previous predicted tokens, predicts the current token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a288741c",
   "metadata": {},
   "source": [
    "### Implementation with RNN\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "RNN, but without output $o_t$.\n",
    "\n",
    "At each time step, only calculates encoder hidden state $h_t = f_{summary}(x_t, h_{t-1}) \\in \\mathbf{R}^h$ from embedded input token $x_t \\in \\mathbf{R}^v$.\n",
    "\n",
    "In batch form (batch size $n$):\n",
    "\n",
    "$$H_t = \\phi(X_tW_{xh} + H_{t-1}W_{hh} + \\mathbf{b}_h) \\in \\mathbf{R}^{n \\times h}$$\n",
    "\n",
    "The generated state is just $\\mathbf{c} = H_T$.\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "RNN, but with 2 modifications, both of which have 2 design choices:\n",
    "\n",
    "1. At each time step $t$, use actual embedded target token $y_{t-1}$ as input (\"teacher-forcing\", only when training), or use model's prediction $o_{t-1}$\n",
    "\n",
    "2. Concatnate additional input $\\mathbf{c}$ to every time step, or only use $\\mathbf{c}$ as an initial hidden state $h_0$\n",
    "\n",
    "Both choosing the former, our modified RNN calculates decoder hidden state $s_t = f_{summary}(y_{t-1}, \\mathbf{c}, s_{t-1}) \\in \\mathbf{R}^s$ and $o_t = f_{predict}(s_t) \\in \\mathbf{R}^v$.\n",
    "\n",
    "In batch form (batch size $n$):\n",
    "\n",
    "$$Y_{t-1} = \\begin{bmatrix} Y_{t-1} & H_T \\end{bmatrix} \\in \\mathbf{R}^{n \\times (v+h)}$$\n",
    "\n",
    "$$S_t = \\phi(Y_{t-1}W_{xs} + S_{t-1}W_{ss} + \\mathbf{b}_s) \\in \\mathbf{R}^{n \\times s}$$\n",
    "\n",
    "$$O_t = S_tW_{sp} + \\mathbf{b}_p \\in \\mathbf{R}^{n \\times v}$$\n",
    "\n",
    "And during testing, no \"teacher-forcing\" can be done, so the previous output $o_{t-1}$ has to be used as input:\n",
    "\n",
    "$$O_{t-1} = \\begin{bmatrix} O_{t-1} & H_T \\end{bmatrix} \\in \\mathbf{R}^{n \\times (v+h)}$$\n",
    "\n",
    "$$S_t = \\phi(O_{t-1}W_{xs} + S_{t-1}W_{ss} + \\mathbf{b}_s) \\in \\mathbf{R}^{n \\times s}$$\n",
    "\n",
    "$$O_t = S_tW_{sp} + \\mathbf{b}_p \\in \\mathbf{R}^{n \\times v}$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
